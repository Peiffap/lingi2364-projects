\documentclass{sigkddExp}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{mathtools}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{mathrsfs}

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{dateplot}
\usepackage{tikzscale}
\usepackage[backend=biber, style=trad-abbrv]{biblatex}
\makeatletter
\def\blx@maxline{77}
\makeatother

\addbibresource{bibliography.bib}

\usepackage[binary-units=true]{siunitx}

\newcommand{\py}[1]{\mintinline{python}{#1}}
\newcommand{\java}[1]{\mintinline{java}{#1}}
\newcommand{\sql}[1]{\mintinline{sql}{#1}}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\theta}{\vartheta}
\renewcommand{\kappa}{\varkappa}
\renewcommand{\rho}{\varrho}
\renewcommand{\phi}{\varphi}

\newcommand{\db}{\mathcal{D}}
\newcommand{\is}{\mathcal{I}}
\newcommand{\tr}{\mathcal{T}}

\newcommand{\abs}[1]{|#1|}
\makeatletter
\@ifundefined{thepage}{\def\thepage{\arabic{page}}}{}%
\makeatother
\usepackage{csquotes}
\usepackage{hyperref}

\newcommand{\ps}{\textsf{PrefixSpan}}
\newcommand{\cs}{\textsf{CloSpan}}
\newcommand{\spade}{\textsf{SPADE}}

\newcommand{\wracc}{\mathrm{WRAcc}}
\newcommand{\abswracc}{\mathrm{AbsWRAcc}}
\newcommand{\ig}{\mathrm{IG}}
\newcommand{\imp}{\mathrm{imp}}

% Setup the mathb font (from mathabx.sty)
\DeclareFontFamily{U}{mathb}{\hyphenchar\font45}
\DeclareFontShape{U}{mathb}{m}{n}{
	<5> <6> <7> <8> <9> <10> gen * mathb
	<10.95> mathb10 <12> <14.4> <17.28> <20.74> <24.88> mathb12
}{}
\DeclareSymbolFont{mathb}{U}{mathb}{m}{n}

% Define a sqsupseteq character from that font (from mathabx.dcl)

\DeclareMathSymbol{\sqsupsetneq}    {3}{mathb}{"89}

\newenvironment{absolutelynopagebreak}
{\par\nobreak\vfil\penalty0\vfilneg
	\vtop\bgroup}
{\par\xdef\tpd{\the\prevdepth}\egroup
	\prevdepth=\tpd}

\numberofauthors{2}

\title{Mining Patterns in Data}
\subtitle{Implementing Sequence Mining}
\author{
\alignauthor Gilles Peiffer (23421600)\\
	\affaddr{Université catholique de Louvain}\\
	\email{gilles.peiffer@student.uclouvain.be}
\alignauthor Liliya Semerikova (64811600)\\
	\affaddr{Université catholique de Louvain}\\
	\email{liliya.semerikova@student.uclouvain.be}}
\date{April 17, 2020}

\begin{document}

\maketitle

\begin{abstract}
	The following paper contains a detailed analysis of the performance and results of using various sequential pattern mining algorithms to fulfill several important tasks in the field of data mining.
	In a first part of the paper, algorithms existing in the literature are described, which are then compared on different tasks.
	The second part of the paper looks at the results of these mining tasks and gathers insights based on them.
\end{abstract}

\section{Introduction}
Frequent sequential pattern mining is an active area of research in data mining with broad applications.
Finding efficient algorithms for this task is thus majorly important, and with the rise of machine learning techniques such as supervised learning, it is interesting to consider which algorithms are able to combine both tasks well.

Additionally, some large datasets contain a lot of redundant information: consider the database made of the single sequence \(\langle (a_1)(a_2) \dots (a_{100}) \rangle\).
With a minimum support of 1, it will generate \(2^{100} - 1\) frequent subsequences, all of which are redundant except for the last one because they have the same support.
For this reason, we also consider algorithms which perform well when mining closed sequential patterns.

\section{Tasks}
\label{sec:tasks}
\subsection{Frequent Sequence Mining}
\label{sec:fsm}
The goal of this task is, given two datasets of respectively positive and negative examples, to find the top-\(k\) most frequent sequential patterns across both of them.
If multiple patterns obtain the same total support, all of them should only count for 1 in the value of \(k\).

\subsection{Supervised Sequence Mining}
\label{sec:ssm}
In supervised sequence mining, the aim is still to find top-\(k\) most frequent patterns, but with a new scoring function instead of the total support.
Let \(p(\alpha)\) and \(n(\alpha)\) be the support of sequential pattern \(\alpha\) in both datasets, and \(P\) and \(N\) be the number of transactions in each dataset; in that case, the \emph{weighted relative accuracy} is given by
\begin{equation}
\wracc(\alpha) \triangleq \frac{PN}{(P + N)^2} \left(\frac{p(\alpha)}{P} - \frac{n(\alpha)}{N}\right).
\end{equation}

In order to search for frequent patterns efficiently, an upper bounding procedure is necessary.
By computing this bound, one can prune the search tree as soon as the bound does not exceed or equal the lowest score found amongst the current top-\(k\) sequential patterns.
It is easy to see that for a sequential pattern \(\beta \sqsupseteq \alpha\), the highest possible \(\wracc\) score that can be attained is bounded by
\begin{equation}
\wracc(\beta) \leqslant \frac{N p(\alpha)}{(P + N)^2}, \quad \forall \beta \sqsupseteq \alpha,
\end{equation}
where the assumption is made that all transactions containing \(\alpha\) in the positive dataset also contain \(\beta\), yet none of the transactions in the negative dataset do.

\subsection{Supervised Closed Sequence Mining}
\label{sec:scsm}
For our purposes, we define a closed sequential pattern \(\alpha\) as a sequence such that for any sequence \(\beta \sqsupsetneq \alpha\), \(p(\alpha) > p(\beta)\) or \(n(\alpha) > n(\beta)\), that is, no supersequence exists which has the same support in both datasets.

The task of supervised closed sequence mining is applied using three different scoring functions:
\begin{itemize}
	\item The \(\wracc\) scoring function described earlier.
	\item The \(\abswracc\) scoring function, defined as
	\begin{equation}
	\abswracc(\alpha) \triangleq \abs{\wracc(\alpha)}.
	\end{equation}
	
	Upper bounding can then be done as follows: for a sequential pattern \(\beta \sqsupseteq \alpha\), the highest possible \(\abswracc\) score that can be attained is bounded by
	\begin{equation}
	\abswracc(\beta) \leqslant \frac{\max\{N p(\alpha), P n(\alpha)\}}{(P + N)^2}, \ \forall \beta \sqsupseteq \alpha.
	\end{equation}
	\item The information gain function, suggested by Quinlan~\cite{Quinlan1986} (where \(p\) and \(n\) are used instead of \(p(\alpha)\) and \(n(\alpha)\), to alleviate notations):
	\begin{multline}
	\ig(\alpha) \triangleq \imp\left(\frac{P}{P + N}\right) - \frac{p + n}{P + N}\, \imp\left(\frac{p}{p + n}\right) \\
	{} - \frac{P + N - p - n}{P + N}\, \imp\left(\frac{P - p}{P + N - p - n}\right),
	\end{multline}
	where \(\imp\) is the entropy, defined as
	\begin{equation}
	\imp(x) \triangleq - x \lg x - (1-x) \lg (1-x).
	\end{equation}
	
	Computing a bound for this scoring function is harder to do analytically.
	To compute the maximum score for a sequential pattern \(\beta \sqsupseteq \alpha\), one has the following relationship:
	\begin{equation}
	\ig(\beta) \leqslant \max_{\substack{0 \leqslant \pi \leqslant p(\alpha) \\ 0 \leqslant \nu \leqslant n(\alpha)}} \ig(\pi, \nu), \quad \forall \beta \sqsupseteq \alpha,
	\end{equation}
	where we have used another definition of the information gain function directly taking the supports of the sequence as arguments.
	By precomputing the information gain for all pairs of values, this bound can be computed as a cumulative maximum.
\end{itemize}

\section{Algorithms}
Various algorithms and implementations were used to complete the tasks outlined in Section~\ref{sec:tasks}.

\subsection{PrefixSpan}
The \ps{} algorithm was proposed by Pei et al.~\cite{Pei2001, Pei2004} in order to mine sequential patterns using a pattern-growth approach.
This algorithm was used for tasks~\ref{sec:fsm} and~\ref{sec:ssm}.
For our purposes, two implementations of this algorithm were written, one using a priority queue or heap to store the top-\(k\) patterns and one using a sorted list.

The algorithm is ran with a value of \(k\) starting from 1 all the way up to the original value, since this strategy allowed for faster pruning on large datasets; for larger values of \(k\), the algorithm would occasionally insert low-scoring patterns in its results list, which would then prevent the algorithm from efficiently pruning its search tree.
By using an incremental strategy, we were able to stop this from happening, with minimal overhead on easier datasets.

\subsection{SPADE}
The \spade{} algorithm was first proposed by Zaki~\cite{Zaki2001}, to solve the problem of frequent sequence mining.
For our survey, it was used to solve task~\ref{sec:fsm}.
It uses a depth-first approach, and is similar to the \textsf{ECLAT} algorithm, also proposed by Zaki~\cite{Zaki2000}.

\subsection{CloSpan}
\label{sec:clospan}
The \cs{} algorithm is an adaptation of \ps{} designed specifically to mine closed sequential patterns proposed by Yan et al.~\cite{Yan2003}, as in task~\ref{sec:scsm}.
In order to do so, the \cs{} algorithm runs in two phases:
\begin{itemize}
	\item A \emph{search} phase, during which \ps{} is run and the following score is computed as \(\sum_{(t, p) \in \db|_\alpha} \db[t] - p + 1\), where \(\db|_\alpha\) is the \(\alpha\)-projected database.
	For any pattern, we check whether a pattern has already been seen that has the same score while also being a supersequence.
	If so, we cut the search tree and backtrack.
	\item A \emph{post-processing} phase, where all patterns which are not closed are removed.
\end{itemize}

As with \ps, in order to avoid exploring the search tree too much because of inefficient bounding, the algorithm is run incrementally.

\section{Performance and Analysis of Resulting Patterns}
\subsection{Frequent Sequence Mining}
\label{sec:perf_fsm}
Figures~\ref{fig:time_sumsup_reuters} and~\ref{fig:time_sumsup_protein} show a comparison of execution times for the frequent sequence mining tasks, respectively on the ``Reuters'' and ``Protein'' datasets.
One can observe that the \ps{} implementations are more memory-efficient.
This can be explained by the fact that \spade{} is more straightforward, whereas \ps{} uses a more intelligent pattern-growth principle to generate new candidates.
Figures~\ref{fig:size_sumsup_reuters} and~\ref{fig:size_sumsup_protein} compare their maximal memory usage.
On this figure and on the next ones, the spike for very low values of \(k\) is presumably a consequence of other tasks running in parallel, which are very visible due to the short execution time for these tasks.
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/length_reuters.tikz}
	\caption{Average pattern length on the ``Reuters'' dataset, using different scoring functions.}
	\label{fig:length_reuters}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_sumsup_reuters.tikz}
	\caption{Execution time comparison for the ``Reuters'' dataset, for the frequent sequence mining task.}
	\label{fig:time_sumsup_reuters}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_sumsup_reuters.tikz}
	\caption{Maximal memory usage comparison for the ``Reuters'' dataset, for the frequent sequence mining task.}
	\label{fig:size_sumsup_reuters}
\end{figure}

By observing the output patterns in this task on Figures~\ref{fig:length_reuters} and~\ref{fig:length_protein}, one can see that shorter patterns always obtain higher support scores.
While this can be useful, Section~\ref{sec:perf_ssm} shows that this is very different from the results obtained when trying to obtain information from the patterns that are being selected.
\subsection{Supervised Sequence Mining}
\label{sec:perf_ssm}
Figures~\ref{fig:time_wracc_reuters} and~\ref{fig:time_wracc_protein} show a comparison of execution times for the frequent sequence mining tasks, respectively on the ``Reuters'' and ``Protein'' datasets.
Figures~\ref{fig:size_wracc_reuters} and~\ref{fig:size_wracc_protein} compare their maximal memory usage.
Both implementations of \ps{} are nearly identical in both aspects, except for some outliers at lower values of \(k\).
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_wracc_reuters.tikz}
	\caption{Execution time comparison for the ``Reuters'' dataset, for the supervised sequence mining task.}
	\label{fig:time_wracc_reuters}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_wracc_reuters.tikz}
	\caption{Maximal memory usage comparison for the ``Reuters'' dataset, for the supervised sequence mining task.}
	\label{fig:size_wracc_reuters}
\end{figure}

Weighted relative accuracy allows one to only keep those sequences which are strongly represented in the positive dataset, but not in the negative dataset.
One possible issue with this is that sequences which appear a lot in the negative dataset but not the positive dataset are ignored, despite giving a lot of information.
\(\abswracc\) and information gain, as explored in Section~\ref{sec:perf_scsm}, can help solve this problem.
When comparing the results with those of Section~\ref{sec:perf_scsm}, one can also observe a lot of superfluous information is present in the output of the non closed algorithms.
By only considering closed patterns, this effect can be reduced, as one gets a lossless representation of the dataset.
As expected, the average pattern length is also longer than for the sum of supports scoring function of Section~\ref{sec:perf_fsm}, which is apparent on Figures~\ref{fig:length_reuters} and~\ref{fig:length_protein}.

\subsection{Supervised Closed Sequence Mining}
\label{sec:perf_scsm}
As mentioned in Section~\ref{sec:perf_ssm}, closed patterns are a lossless representation of the dataset.
Figures~\ref{fig:time_closed_wracc_reuters},~\ref{fig:time_closed_abswracc_reuters}, and~\ref{fig:time_closed_infogain_reuters} give the execution for the three scoring functions for the supervised closed sequence mining task on the ``Reuters'' dataset, while Figures~\ref{fig:size_closed_wracc_reuters},~\ref{fig:size_closed_abswracc_reuters}, and~\ref{fig:size_closed_infogain_reuters} give the maximal memory consumption.
Figures~\ref{fig:time_closed_wracc_protein} through~\ref{fig:size_closed_infogain_protein} do the same for the ``Protein'' dataset.
As one can see on these figures for the ``Reuters'' dataset, there seems to be an apparent memory overhead for low values of \(k\) when using the implementation with a sorted list, though this difference disappears for higher values of \(k\).
The inverse effect appears on the ``Protein'' dataset, with the heap implementation using more memory.
This leads us to believe that certain properties of the dataset might affect the various algorithms differently.
Another observation is that for the information gain scoring function on the ``Reuters'' dataset, which requires a lot of precomputation and is thus slower to compute even for small values of \(k\), the spike which could be seen on other memory usage figures is gone, which corroborates our belief that this spike is due to noise on the measurements as a consequence of low execution time, rather than an intrinsic property of the algorithms.
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_wracc_reuters.tikz}
	\caption{Execution time comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the \(\wracc\) scoring function.}
	\label{fig:time_closed_wracc_reuters}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_wracc_reuters.tikz}
	\caption{Maximal memory usage comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the \(\wracc\) scoring function.}
	\label{fig:size_closed_wracc_reuters}
\end{figure}
On the figures for closed sequence mining with the \(\wracc\) scoring function, one can observe that using closed sequences is slightly faster than not doing so, which might be due to the additional pruning of the search tree as described in Section~\ref{sec:clospan}.
The patterns that are obtained are a subset of the ones obtained previously, when mining non closed sequences as well.

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_abswracc_reuters.tikz}
	\caption{Execution time comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the \(\abswracc\) scoring function.}
	\label{fig:time_closed_abswracc_reuters}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_abswracc_reuters.tikz}
	\caption{Maximal memory usage comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the \(\abswracc\) scoring function.}
	\label{fig:size_closed_abswracc_reuters}
\end{figure}
Performance-wise, the \(\abswracc\) scoring function does not make a big difference compared to the original \(\wracc\) function, but it is a much better guide in determining whether a given pattern brings a lot of information with regards to either class in the dataset.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_infogain_reuters.tikz}
	\caption{Execution time comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the information gain scoring function.}
	\label{fig:time_closed_infogain_reuters}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_infogain_reuters.tikz}
	\caption{Maximal memory usage comparison for the ``Reuters'' dataset, for the supervised closed sequence mining task with the information gain scoring function.}
	\label{fig:size_closed_infogain_reuters}
\end{figure}

As with the \(\abswracc\) scoring function, the information gain is a way of measuring the quality of a pattern in a supervised environment.
However, it is often slower, due to its weaker upper-bound (depsite the latter being optimal).
Often, both functions give similar results, though this is not always the case.
Choosing which scoring function to use thus depends on problem-specific properties.

\section{Conclusion}
Sequence mining, whether supervised or not, is a hugely important task, with broad applications.
By combining it with supervised learning, it becomes even more useful, as sequence mining can be used to obtain information about multiple datasets, and to find defining features of each, according to various scoring functions.
Closed sequence mining is useful in that it provides the user with a lossless representation of the dataset.

\newpage
\printbibliography

\appendix

\section{Omitted Figures}
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/length_protein.tikz}
	\caption{Average pattern length on the ``Protein'' dataset, using different scoring functions.}
	\label{fig:length_protein}
\end{figure}%
\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_sumsup_protein.tikz}
	\caption{Execution time comparison for the ``Protein'' dataset, for the frequent sequence mining task.}
	\label{fig:time_sumsup_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_sumsup_protein.tikz}
	\caption{Memory usage comparison for the ``Protein'' dataset, for the frequent sequence mining task.}
	\label{fig:size_sumsup_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_wracc_protein.tikz}
	\caption{Execution time comparison for the ``Protein'' dataset, for the supervised sequence mining task.}
	\label{fig:time_wracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_wracc_protein.tikz}
	\caption{Memory usage comparison for the ``Protein'' dataset, for the supervised sequence mining task.}
	\label{fig:size_wracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_wracc_protein.tikz}
	\caption{Execution time comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the \(\wracc\) scoring function.}
	\label{fig:time_closed_wracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_wracc_protein.tikz}
	\caption{Maximal memory usage comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the \(\wracc\) scoring function.}
	\label{fig:size_closed_wracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_abswracc_protein.tikz}
	\caption{Execution time comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the \(\abswracc\) scoring function.}
	\label{fig:time_closed_abswracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_abswracc_protein.tikz}
	\caption{Maximal memory usage comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the \(\abswracc\) scoring function.}
	\label{fig:size_closed_abswracc_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/time_closed_infogain_protein.tikz}
	\caption{Execution time comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the information gain scoring function.}
	\label{fig:time_closed_infogain_protein}
\end{figure}

\begin{figure}[!hbtp]
	\centering
	\includegraphics[width=\linewidth]{plots/size_closed_infogain_protein.tikz}
	\caption{Maximal memory usage comparison for the ``Protein'' dataset, for the supervised closed sequence mining task with the information gain scoring function.}
	\label{fig:size_closed_infogain_protein}
\end{figure}
\end{document}
